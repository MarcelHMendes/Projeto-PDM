{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "154c1aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 18:21:32 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.\\\n",
    "        builder.\\\n",
    "        appName(\"pyspark-notebook\").\\\n",
    "        master(\"spark://spark-master:7077\").\\\n",
    "        config(\"spark.executor.memory\", \"2g\").\\\n",
    "        getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f25413d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"data/final_dataset.csv\")\n",
    "#df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263743af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b84da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install numpy\n",
    "!{sys.executable} -m pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "620e309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split, explode, col, desc\n",
    "from pyspark.ml import Pipeline \n",
    "from pyspark.ml.feature import VectorAssembler \n",
    "from pyspark.ml.feature import StringIndexer \n",
    "from pyspark.ml.classification import NaiveBayes \n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "indexers = [\n",
    "StringIndexer(inputCol=\"Dst IP\", outputCol = \"dstip_index\"),\n",
    "StringIndexer(inputCol=\"Dst Port\", outputCol = \"dstport_index\"),\n",
    "StringIndexer(inputCol=\"FIN Flag Cnt\", outputCol = \"fin_index\"),\n",
    "StringIndexer(inputCol=\"SYN Flag Cnt\", outputCol = \"syn_index\"),\n",
    "StringIndexer(inputCol=\"URG Flag Cnt\", outputCol = \"urg_index\"),\n",
    "StringIndexer(inputCol=\"PSH Flag Cnt\", outputCol = \"cwe_index\"),\n",
    "StringIndexer(inputCol=\"ECE Flag Cnt\", outputCol = \"ece_index\"),\n",
    "StringIndexer(inputCol=\"Ack Flag Cnt\", outputCol = \"ack_index\"),\n",
    "StringIndexer(inputCol=\"Label\", outputCol = \"label\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdd6ec47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 18:21:59 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers)\n",
    "df = df.select(\"Dst IP\", \n",
    "               \"Dst Port\",\n",
    "               \"FIN Flag Cnt\", \n",
    "               \"SYN Flag Cnt\", \n",
    "               \"URG Flag Cnt\",\n",
    "               \"PSH Flag Cnt\",\n",
    "               \"ECE Flag Cnt\",\n",
    "               \"Ack Flag Cnt\",\n",
    "               \"Label\"\n",
    "              )\n",
    "indexed_rows_df = pipeline.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b57be869",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols = [\"dstip_index\",\"dstport_index\",\"fin_index\", \"syn_index\", \"urg_index\", \"cwe_index\",\"ece_index\", \"ack_index\"],outputCol = \"features\")\n",
    "vindexed_rows_df = vectorAssembler.transform(indexed_rows_df)\n",
    "#vindexed_rows_df.show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80e860f",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = vindexed_rows_df.randomSplit([0.6,0.4], 42) \n",
    "# optional value 42 is seed for sampling \n",
    "train_df = splits[0] \n",
    "test_df = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "671974ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 18:29:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:30:48 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:30:50 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:30:54 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:30:55 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/02/20 18:30:55 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/02/20 18:30:55 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:30:59 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:00 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:03 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:04 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:08 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:09 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:13 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:13 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:17 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:18 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:22 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:25 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:26 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:30 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:34 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:34 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "22/02/20 18:31:38 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55cc785d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df = lrModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad4c5e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 18:33:35 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy = 0.9055975994071201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/20 18:35:21 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "[Stage 45:======================================================> (50 + 1) / 51]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score = 0.9055469891871994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\") \n",
    "nbaccuracy = evaluator.evaluate(predictions_df) \n",
    "print(\"Test accuracy = \" + str(nbaccuracy))\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "nbf1 = evaluator.evaluate(predictions_df) \n",
    "print(\"F1 score = \" + str(nbf1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba887b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
